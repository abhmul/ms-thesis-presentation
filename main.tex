\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}

\usetheme{Madrid}
\usecolortheme{default}

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}


%------------------------------------------------------------
%This block of code defines the information to appear in the
%Title page
\title[An Application of Martingales] %optional
{An Application of Martingales}

\subtitle{Components of Erdos-Renyi Graphs}

\author[Mulgund, Abhijeet] % (optional)
{A.~Mulgund\inst{1}}

\institute[UIC] % (optional)
{
  \inst{1}%
  Mathematics, Statistics, and Computer Science\\
  University of Illinois at Chicago
}

% \date[VLC 2021] % (optional)
% {Very Large Conference, April 2021}

% \logo{\includegraphics[height=1cm]{overleaf-logo}}

%End of title page configuration block
%------------------------------------------------------------



%------------------------------------------------------------
%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}
%------------------------------------------------------------


\begin{document}

%The next statement creates the title page.
\frame{\titlepage}


%---------------------------------------------------------
%This block of code is for the table of contents after
%the title page
\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}
%---------------------------------------------------------


\section{Formulating the Problem}

%---------------------------------------------------------
%Changing visivility of the text
\begin{frame}
\frametitle{Asymptotically Almost Surely}
We would like to study discrete stochastic processes and characterize their behavior asymptotically almost surely.

\begin{definition}
    Let \((X_{n}: \Omega \to \mathbb{R})_{n \geq 1}\) be a stochastic process and let \((x_{n})_{n \geq 1}\subset \mathbb{R}\). Then \(X_{n} = x_{n}\) \textbf{asymptotically almost surely} if there exists \((\epsilon_{n})_{n \geq 1} \subset \mathbb{R}\) so that \(\epsilon_{n} = o(1)\) and
    \[\mathbb{P}((1 - \epsilon_{n})x_{n} \leq X_{n} \leq (1 + \epsilon_{n})x_{n}) \to 1\]
    as \(n \to \infty\). It may be abbreviated a.a.s..
\end{definition}

\pause We can also say events \(\mathcal{E}_{n}\) a.a.s. if \(\mathbb{P}(\mathcal{E}_{n}) \to  1\)

\end{frame}

%---------------------------------------------------------


%---------------------------------------------------------
%Example of the \pause command
\begin{frame}
    Some examples of a.a.s. events
    \begin{itemize}
        \item<1-> An integer is composite a.a.s.
        \item<2-> Consider tossing \(m_{n} = \frac{1}{3}n\log n\) balls into \(n\) bins. The number of bins with exactly \(k\) balls is a.a.s. \[\frac{\frac{m_{n}}{n}^{k} e^{-m_{n}/n}}{k!}n\] \pause
    \end{itemize}
\pause
    \begin{theorem}
        Let \(\kappa \geq 1\) and \(c \in \mathbb{R}^{\geq 0}\). Then a.a.s. the number of connected components of order \(k\), for \(1 \leq k \leq \kappa\), in \(G_{n, \lfloor cn \rfloor}\) is
    \[\frac{k^{k-2}}{k!}(2c)^{k-1} e^{-2kc}n\]
    \end{theorem}
\pause
This theorem is the one we'll focus on proving.
\end{frame}
%---------------------------------------------------------

\section{Useful Theorems}

%---------------------------------------------------------
%Highlighting text
\begin{frame}
\frametitle{Erdos-Renyi Process}

To study \(G_{n, m}\), we'll instead look at the process of sampling a new edge to add to our graph until we have \(m\) steps. \\~\
\pause

We'll call this the Erdos-Renyi Process. \\~\ \pause

At each step \(0 \leq i \leq m\), we'll have
\begin{itemize}
    \item Sigma Field \(\mathcal{F}_{i}\) generated by all the possible first \(i\) choices of edges. These will form a filtration. \pause
    \item \(G_{n, i}\), the (random) graph we observe after drawing \(i\) edges. \pause
    \item \(Y_{k}(i)\), the (random) number of components in \(G_{n, i}\)
\end{itemize}

\end{frame}
%---------------------------------------------------------


%---------------------------------------------------------
%Two columns
\begin{frame}
\frametitle{Basic Idea}

We'll approximate discrete \(Y_{k}\) with continuous \(ny_{k} \in C^{2}[0, 1]\). That is,
\[n(y_{k}(t_{i}) + \epsilon_{n}(t_{i})) = Y_{k}(i)\]
for \(t_{i} = \frac{i}{n}\) and \(\epsilon_{n} = o(y_{k}(t_{i})) \). \\~\

\pause
To do this we need
\begin{itemize}
  \item good guesses for \(y_{k}\) and \(\epsilon_{n}\) \pause
  \item a way to show the that \(Y_{k}\) will a.a.s. not stray far from our approximation.
\end{itemize}


\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}
  \frametitle{Azuma-Hoeffding}
  
  We will construct a supermartingale out of our deviation of \(Y_{k}\) from our approximation. Then we can apply
  \pause
  \begin{theorem}[Azuma-Hoeffding's Inequality]
    \label{azuma}
    Let \((X_{n})_{n \geq 0}\) be a supermartingale and suppose there exists real \(C \geq 0\) \(|X_{n+1} - X_{n}| \leq C\) a.s. for \(n \geq 0\). Then, for all \(\lambda \in \mathbb{R}^{\geq 0}\) and \(m \geq 1\),
    \[\mathbb{P}(X_{m} - X_{0} \geq  \lambda) \leq \exp\left(-\frac{\lambda^{2}}{2C^{2}m}\right)\]
  \end{theorem}
  \pause
  Since our \(m = O(n)\), we only need a \(\lambda = \omega(n^{1/2})\). This observation will help guide our choice for \(\epsilon_{n}\).
  
  
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
\begin{frame}
  \frametitle{Taylor's Theorem}
  
  We also need to an easy way to analyze discrete time steps in \(y_{k}(t)\) and \(\epsilon_{n}(t)\). Here we can apply
  \pause
  \begin{theorem}[Taylor's Theorem]
    \label{taylor}
    Let \(f \in C^{2}([a, b])\) for \(a < b \in \mathbb{R}\). Then there exists \(\tau \in (a, b)\) so that
    \[f(b) = f(a) + f'(a)(b-a) + \frac{f''(\tau)}{2}(b-a)^{2}.\]
  \end{theorem}
  \pause
  A step from \(i\) to \(i+1\) amounts to a step of \(\frac{1}{n}\) in \(t\), so \(b-a\) will be \(\frac{1}{n}\). In our computations, this will render the 2nd derivative term negligible.
  
  
\end{frame}
%---------------------------------------------------------

\section{Creating the Supermartingale}

%---------------------------------------------------------
%Highlighting text
\begin{frame}
\frametitle{Freezing the Error}

We want to restrict our attention to when \(Y_{k}\) is controlled. \pause If we can show that \(Y_{k}\) becomes less likely to venture out of our "nice" region as \(n \to \infty\), then we're good. \\~\

\pause
Let 
\[\nu_{n}^{\pm} := \inf\limits \{i \geq 0 : \pm (Y_{k}(i) - n(y_{k}(t_{i})) > \epsilon_{n}(t_{i}))\}.\]
This is the first time \(Y_{k}\) ventures outside our "nice" zone.\\~\

\pause
Define
\[Y_{k}^{\pm}(i) = Y_{k}(i \wedge \nu_{n}^{\pm}) - n(y_{k}(t_{i \wedge \nu_{n}^{\pm}}) \pm \epsilon_{n}(t_{i \wedge \nu_{n}^{\pm}})). \]
We are freezing \(Y_{k}\) the moment it leaves the nice zone. \\~\

\pause
With the right choice of \(y_{k}\) and \(\epsilon_{n}\), \(Y_{k}^{+}\) will be a supermartingale.

\end{frame}
%---------------------------------------------------------


\section{Expected Change}

%---------------------------------------------------------
%Highlighting text
\begin{frame}
\frametitle{Changes to Component Counts}

Let \(\Delta Y_{k}(i) = Y_{k}(i+1) - Y_{k}(i)\). Given the first \(i\) choices of edges (\(\mathcal{F}_{i}\)), we have the possible outcomes for the new edge \(e_{i+1}\):
\pause
\begin{enumerate}
    \item \(e_{i+1}\) connects a component of size \(k\) to one of size that's not \(k\). Then \pause 
      \begin{itemize}
        \item there are \(k Y_{k}(i) (n - k Y_{k}(i))\) such edges \pause
        \item \(\Delta Y_{k}(i) = -1\) \pause
      \end{itemize}
    \item \(e_{i+1}\) connects a component of size \(k\) to another component of size \(k\). Then \pause
    \begin{itemize}
      \item there are \(\frac{1}{2} k^{2} (Y_{k}(i) - 1) Y_{k}(i)\) such edges \pause
      \item \(\Delta Y_{k}(i) = -2\) \pause
    \end{itemize}
    \item \(e_{i+1}\) connects a component of size \(j\) and size \(k - j\). Then \pause
    \begin{itemize}
      \item there are \(j Y_{j}(i) (k - j) Y_{k-j}(i)\) such edges \pause
      \item \(\Delta Y_{k}(i) = 1\) \pause
    \end{itemize}
    \item Otherwise, \(\Delta Y_{k}(i) = 0\).
\end{enumerate}
\end{frame}
%---------------------------------------------------------


%---------------------------------------------------------
%Two columns
\begin{frame}
\frametitle{Changes to Component Counts}

There are \(\binom{n}{2} - i\) possible choices for edges. \\~\ \pause

Choosing an edge not yet picked has probability
\[\frac{1}{\binom{n}{2} - i} = \frac{2}{n^{2} + n - 2i} \leq \frac{2}{n^{2}} \left(\frac{n^{2}}{n^{2} - n} \right) =  \frac{2}{n^{2}} \left(1 + O(\frac{1}{n})\right)\]
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Two columns
\begin{frame}
\frametitle{Changes to Component Counts}
First looking at the contribution of cases 1 and 2 to \(\mathbb{E}(\Delta Y_{k}(i)|\mathcal{F}_{i})\)
\[
  % \begin{cases}
    \begin{aligned}
      &-  \frac{2}{n^{2}} \left(1 + O(\frac{1}{n})\right) \Big[  n k Y_{k}(i) - k^{2} Y_{k}(i)^{2}   + k^{2} Y_{k}(i)^{2} - k^{2} Y_{k}(i)\Big] \\ \pause
      =&  -  \frac{2}{n^{2}} \left(1 + O(\frac{1}{n})\right) \Big[  n k Y_{k}(i) - k^{2} Y_{k}(i)\Big] \\ \pause
      =& \left(1 + O(\frac{1}{n})\right)  \Big[  -2 k \frac{Y_{k}(i)}{n} + k^{2} \frac{2Y_{k}(i)}{n^{2}}\Big] \\ \pause
      \leq& \left(1 + O(\frac{1}{n})\right)  \Big[  -2 k \frac{Y_{k}(i)}{n} + \kappa^{2} \frac{2}{n}\Big] \\ \pause
      \leq& -2 k \frac{Y_{k}(i)}{n} + O(\frac{1}{n}) + O(\frac{1}{n^{2}}) \\ \pause
      =& -2 k \frac{Y_{k}(i)}{n} + O(\frac{1}{n}) \\ \pause
    \end{aligned}
  % \end{cases}
\]
We make use of the fact that \(Y_{k}(i) \leq n\) and \(k \leq \kappa\).

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Two columns
\begin{frame}
  \frametitle{Changes to Component Counts}
  Next, we look at the contribution of case 3:
  \[
    \begin{aligned}
      & \frac{2}{n^{2}} \left(1 + O(\frac{1}{n})\right) \frac{1}{2}\sum\limits_{j=1}^{k-1} j Y_{j}(i) (k - j) Y_{k-j}(i) \\ \pause
      &=  \left(1 + O(\frac{1}{n})\right) \sum\limits_{j=1}^{k-1} j (k - j) \frac{Y_{j}(i)}{n} \frac{Y_{k-j}(i)}{n} \\ \pause
      &\leq \sum\limits_{j=1}^{k-1} \Big[  j (k - j) \frac{Y_{j}(i)}{n}  \frac{Y_{k-j}(i)}{n} \Big] + \kappa^{3} O(\frac{1}{n}) \\ \pause
      &=\sum\limits_{j=1}^{k-1} \Big[  j (k - j) \frac{Y_{j}(i)}{n}  \frac{Y_{k-j}(i)}{n} \Big]+ O(\frac{1}{n}) \pause
    \end{aligned}
  \]
  where we again use the fact that \(Y_{j}(i), Y_{k-j}(i) \leq n\) and \(j, k-j \leq \kappa\).

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Two columns
\begin{frame}
  \frametitle{Changes to Component Counts}
  Assuming \(i < \nu_{n}^{+}\), we have that
  \[
    \begin{aligned}
      \mathbb{E}&(\Delta Y_{k}(i)|\mathcal{F}_{i}, [i < \nu_{n}^{+}])  \\
      &= -2 k (y_{k}(t_{i}) + \epsilon_{n}(t_{i})) \\
      &\hbox{ }+ \sum\limits_{j=1}^{k-1} \Big[ j (k - j) (y_{j}(t_{i}) + \epsilon_{n}(t_{i})) (y_{k-j}(t_{i}) + \epsilon_{n}(t_{i})) \Big] + O(\frac{1}{n}) \\ \pause
      &\leq -2 k y_{k}(t_{i}) + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}(t_{i})y_{k-j}(t_{i})\Big] -2 k \epsilon_{n}(t_{i}) + \epsilon_{n}(t_{i}) 3 k^{3} + O(\frac{1}{n}) \\ \pause
      &\leq -2 k y_{k}(t_{i}) + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}(t_{i})y_{k-j}(t_{i})\Big] + \epsilon_{n}(t_{i}) 3 k^{3} + O(\frac{1}{n}) \pause
    \end{aligned}
  \]
  using the fact that \(y_{k}(t) \leq 1\) and \(0 \leq \epsilon_{n}(t) \leq 1\) for sufficiently large \(n\).
\end{frame}
%---------------------------------------------------------

\section{Finding the Differential Equation}

%---------------------------------------------------------
%Highlighting text
\begin{frame}
Given \([i < \nu_{n}^{+}]\), 
\[\Delta Y_{k}^{+}(i) = \Delta Y_{k}(i) - (y_{k}(t_{i} + \frac{1}{n}) - y_{k}(t_{i}) + \epsilon_{n}(t_{i} + \frac{1}{n}) - \epsilon_{n}(t_{i})).\] \\~\ \pause

Here we use Taylor's Theorem to manage the discrete increment of a continuous function: \pause
\[
  \begin{aligned}
    n(y_{k}(t_{i} + \frac{1}{n}) - y_{k}(t_{i})) &= n(\frac{y_{k}'(t_{i})}{n} + \frac{y_{k}''(\tau)}{n^{2}}) \\ \pause
    &= y_{k}'(t_{i}) + O(\frac{1}{n}) \pause
  \end{aligned}
\]

Similarly,
\[n(\epsilon_{n}(t_{i} + \frac{1}{n}) - \epsilon_{n}(t_{i})) = \epsilon_{n}'(t_{i}) + O(\frac{1}{n})\]
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Highlighting text
\begin{frame}

  Applying the Taylor Theorem simplifications: \pause
  \[
    \begin{aligned}
      \mathbb{E}&(\Delta Y^{+}_{k}(i)|\mathcal{F}_{i}, [i < \nu_{n}^{+}]) \\
      &= -2 k y_{k}(t_{i}) + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}(t_{i})y_{k-j}(t_{i})\Big] - y_{k}'(t_{i}) \\
      &\hbox{ } + \epsilon_{n}(t_{i}) 3 k^{3}  - \epsilon_{n}'(t_{i}) + O(\frac{1}{n}) \pause
    \end{aligned}
  \]
  
  If 
  \[y_{k}'(t) = -2 k y_{k}(t) + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}(t)y_{k-j}(t)\Big]\] \pause
  and for sufficiently large \(n\), 
  \[ \epsilon_{n}(t) 3 k^{3} + O(\frac{1}{n}) \leq \epsilon_{n}'(t)\] \pause
  then \(Y^{+}_{k}\) is a supermartingale.

\end{frame}
%---------------------------------------------------------

\section{Solving the Differential Equations}

%---------------------------------------------------------
%Highlighting text
\begin{frame}
  The easier one to address is \(\epsilon_{n}\). \pause Note that 
  \[3k^{3}e^{4 \kappa^{3}t} \leq 4 \kappa^{3} e^{4 \kappa^{3}t} = \Big(e^{4 \kappa^{3}t} \Big)'\] \pause
  so choosing 
  \[\epsilon_{n}(t) = \omega(\frac{1}{n}) e^{4 \kappa^{3}t}\]
  should do the trick. \pause \\~\

  We'll sort out the \(\omega(\frac{1}{n})\) term later when we apply Azuma-Hoeffding.

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Highlighting text
\begin{frame}
  Addressing \(y_{k}(t)\) is more interesting. \pause We want to solve
  \[y_{k}'(t) = -2 k y_{k}(t) + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}(t)y_{k-j}(t)\Big] \text{ } \forall 1 \leq k \leq \kappa\] \pause
  \begin{claim}
    \(y_{k}(t) = \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt}\) is the unique solution to the ODE system.
  \end{claim}
  
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Highlighting text
\begin{frame}
  \frametitle{Proof of Claim}
  \(k = 1\) can be verified by inspection. \pause For \(k \geq 2\), we get
    \[
      \begin{aligned}
        -2 k &y_{k} + \sum\limits_{j=1}^{k-1} \Big[ j (k - j) y_{j}y_{k-j}\Big] \\
        = &-2 k \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt} \\
        &+ \sum\limits_{j=1}^{k-1} \Big[ j (k - j) \frac{j^{j-2}}{j!}(2t)^{j-1} e^{-2jt} \frac{(k-j)^{(k-j)-2}}{(k-j)!}(2t)^{(k-j)-1} e^{-2(k-j)t}\Big] \\ \pause 
        = &-2 k \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt} + \sum\limits_{j=1}^{k-1} \Big[  \frac{j^{j-1}(k-j)^{(k-j)-1} }{j!(k-j)!}(2t)^{k-2} e^{-2kt}\Big] \\ \pause
        = &-2 k \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt} + (2t)^{k-2} \frac{e^{-2kt}}{k!} \sum\limits_{j=1}^{k-1} \Big[ \binom{k}{j} j^{j-1}(k-j)^{(k-j)-1}\Big] \pause
    \end{aligned}
  \]
  
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Highlighting text
\begin{frame}
  \frametitle{Proof of Claim}
  If
    \[\sum\limits_{j=1}^{k-1} \Big[ \binom{k}{j} j^{j-1}(k-j)^{(k-j)-1}\Big] = 2(k-1)k^{k-2}\]
  then product rule proves the claim. \\~\ \pause

  To show this, we examine labeled trees over \([k]\). \pause For \(1 \leq  j \leq k-1\)
  \begin{enumerate}
      \item we have \(\binom{k}{j}\) choices for subsets \(J \subset [k]\) of size \(|J| = j\) \pause
      \item we have \(j^{j-2}\) choices for labelled trees on \(J\) (Cayley's Formula) \pause
      \item we have \((k-j)^{k-j-2}\) choices for labelled trees on \([k] \setminus J\) \pause
      \item we have \(j(k-j)\) choices for edges between \(J\) and \([k] \setminus J\). \pause
  \end{enumerate}

  So in total \(\sum\limits_{j=1}^{k-1} \Big[ \binom{k}{j} j^{j-2}(k-j)^{(k-j)-2} j(k-j)\Big]\) choices. After completing all 4 choices, we get a labeled tree on \([k]\).
  
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Highlighting text
\begin{frame}
  \frametitle{Proof of Claim}
  We can reverse engineer the 4 choices by \pause
  \begin{enumerate}
    \item picking a labeled tree \(T \subset [k]\) (\(k^{k-2}\) choices) \pause
    \item choosing one of the \(k-1\) edges of \(T\), call it \(e\) \pause
    \item removing edge \(e\) from \(T\) and choosing one of the \(2\) remaining components as \(J\). \pause
\end{enumerate}

So in total \(2(k-1)k^{k-2}\) choices. This is completes the proof. \(\blacksquare\)

\end{frame}
%---------------------------------------------------------

\section{Applying Azuma's Inequality}

%---------------------------------------------------------
%Highlighting text
\begin{frame}
\frametitle{Recap}
So far we've shown that if \pause
\begin{itemize}
  \item \(y_{k}(t) = \frac{k^{k-2}}{k!}(2t)^{k-1} e^{-2kt}\) \pause 
  \item \(\epsilon_{n}(t) = \omega(\frac{1}{n}) e^{4 \kappa^{3}t}\) \pause
\end{itemize}
then \((Y^{+}_{k}(i))_{i=0}^{\lfloor {cn} \rfloor}\) is a supermartingale.

\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Highlighting text
\begin{frame}
  \frametitle{Azuma-Hoeffding Requirements}
  First we need to show \(|\Delta Y_{k}^{+}(i)| = O(1)\) a.s. \(\forall 0 \leq i \leq \lfloor {cn} \rfloor\). \\~\ \pause

  If \(i < \nu_{n}^{+}\)
  \[
    \begin{aligned}
        |\Delta Y_{k}^{+}(i)| &= |\Delta Y_{k}(i) - (y_{k}(t_{i} + \frac{1}{n}) - y_{k}(t_{i}) + \epsilon_{n}(t_{i} + \frac{1}{n}) - \epsilon_{n}(t_{i}))| \\ \pause
        &= |\Delta Y_{k}(i) - y_{k}'(t_{i}) - \epsilon_{n}'(t_{i}) + O(\frac{1}{n})| \\ \pause
        &\leq |\Delta Y_{k}(i)| + |y_{k}'(t_{i})| + |\epsilon_{n}'(t_{i})| + O(\frac{1}{n}) \\ \pause
        &\leq 2 + O(1) + O(1) + O(\frac{1}{n}) \\ \pause
        &= O(1) \pause
    \end{aligned}
  \]
  Otherwise, \(Y_{k}^{+}\) is frozen, so \(|\Delta Y_{k}^{+}(i)| = 0\)
  
\end{frame}
%---------------------------------------------------------

%---------------------------------------------------------
%Highlighting text
\begin{frame}
  \frametitle{Applying Azuma-Hoeffding}
  For step \(m = \lfloor {cn} \rfloor\) we want to bound \(\mathbb{P}(m \geq \nu_{n}^{+}) = \mathbb{P}(Y_{k}^{+}(m) > 0)\). \pause

  We have
  \[
    Y_{k}^{+}(0) = Y_{k}(0) - ny_{k}(0) - n\epsilon_{n}(0) = -n\epsilon_{n}(0)
  \] \pause
  so
  \[
    \begin{aligned}
      \mathbb{P}(&Y_{k}^{+}(m) > 0) \\
      &= \mathbb{P}(Y_{k}^{+}(m) - Y_{k}^{+}(0) > n\epsilon_{n}(0)) \\ \pause
      &\leq  \exp\left( - \frac{ n^{2}\epsilon_{n}(0)^{2} }{2 C^{2}m} \right) \\  \pause
      &\leq \exp\left( - \frac{n \epsilon_{n}(0)^{2}}{2 C^{2}} \right) \\ 
    \end{aligned}
  \]
  where we absorbed \(c\) from \(m = \lfloor {cn} \rfloor\) into \(C^{2}\). \pause \\~\ 
  
  Choosing \(\epsilon_{n}(0) = \omega(n^{-1/2})\), such as \(\epsilon_{n}(t) = n^{-1/3}e^{4 \kappa^{3}t}\), should do the trick. So \(\mathbb{P}(m \geq \nu_{n}^{+}) \to 0\)
  
\end{frame}
%---------------------------------------------------------


%---------------------------------------------------------
%Two columns
\begin{frame}
\frametitle{Symmetric Calculation and Wrap-Up}

A symmetric calculation for \(Y_{k}^{-}\) gets us that \(\mathbb{P}(m \geq \nu_{n}^{-}) \to 0\) as well. \\~\ \pause

Finally, applying a union bound, we get that
\[\mathbb{P}(m \geq \nu_{n}^{+} \text{ or } m \geq \nu_{n}^{-}) \leq \mathbb{P}(m \geq \nu_{n}^{+}) + \mathbb{P}(m \geq \nu^{-}) = o(1) + o(1)\]
\end{frame}
%---------------------------------------------------------

\end{document}